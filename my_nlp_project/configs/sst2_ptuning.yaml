dataset:
  name: sst-2
  path: my_nlp_project/data

plm:
  model_name: bert
  model_path: /root/autodl-tmp/OpenPrompt/bert-base-uncased
  optimize:
    freeze_para: False
    lr: 2e-5
    weight_decay: 0.01
    scheduler:
      type: linear
      num_warmup_steps: 100

train:
  batch_size: 16
  num_epochs: 3

test:
  batch_size: 16

dev:
  batch_size: 16

reproduce:
  seed: 42

template: ptuning_template
verbalizer: manual_verbalizer

ptuning_template:
  choice: 0
  file_path: /root/autodl-tmp/OpenPrompt/my_nlp_project/prompt_resources/SST-2/ptuning_template.txt
  soft_token_num: 20
  encoder_type: lstm
  encoder_hidden_size: 800

manual_verbalizer:
  choice: 0
  file_path: /root/autodl-tmp/OpenPrompt/my_nlp_project/prompt_resources/SST-2/manual_verbalizer.txt

environment:
  num_gpus: 1
  cuda_visible_devices: [0]
  local_rank: 0

learning_setting: few_shot

few_shot:
  parent_config: learning_setting
  few_shot_sampling: sampling_from_train

sampling_from_train:
  parent_config: few_shot_sampling
  num_examples_per_label: 16
  also_sample_dev: False
  seed: [42, 123, 456]

logging:
  path_base: /root/autodl-tmp/OpenPrompt/my_nlp_project/logs
  unique_string: sst2_bert_ptuning_template_manual_verbalizer_16shot
  overwrite: false
